{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic_experiments\n",
    "\n",
    "> Just some experiments with \"RAG\" indexing and semantic search using common toolkits like langchain, llamaindex, P   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp semantic_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on using Proposition Chunking and RAG indexing for semantic search\n",
    "This is based on the [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) notebook example for the \"Agentic Text Splitting\" method. It is based on the [Dense X Retrieval: What Retrieval Granularity Should We Use?](https://arxiv.org/pdf/2312.06648.pdf) paper and has a prompt implementation in LangChain hub. Unfortunately, the LangChain hub is API walled, so I copied the prompt template to this notebook. We need to handle PDFs as a combination of text, image and tabular data for the purposes of chunking and indexing. LangChain blog has some interesting experiments on [Benchmarking RAG on tables](https://blog.langchain.dev/benchmarking-rag-on-tables/), [Multi-modal RAG on slide decks](https://blog.langchain.dev/multi-modal-rag-template/) and a set of notebooks [Multi-modal eval: GPT-4 w/ multi-modal embeddings and multi-vector retriever](https://langchain-ai.github.io/langchain-benchmarks/notebooks/retrieval/multi_modal_benchmarking/multi_modal_eval.html?ref=blog.langchain.dev). The [Unstructured IO](https://github.com/Unstructured-IO/unstructured) project provides multi-document extractors and handling for PDFs with text, tables and images. Langchain document loaders support unstructured.io [Unstructured File](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file?ref=blog.langchain.dev) document loaders. There is a good example in the [Semi-structured RAG cookbook example](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb). For retrieval, there is a [LangChain Cookbook Example](https://python.langchain.com/docs/expression_language/cookbook/retrieval) that demonstrates using an LLM with vector store to answer questions. There is an example of using Unstructured.IO to build KGs using neo4j at Neo4j KG [PDF-KG loader](https://github.com/Joshua-Yu/graph-rag/blob/main/unstructured-io/Unstructured-IO_PDF_KGLoader.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pydantic import BaseModel\n",
    "\n",
    "filename = \"data/DoD_Data_Strategy.pdf\"\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "    extract_images_in_pdf=False,\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\", \n",
    "    model_name=\"yolox\",\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 16,\n",
       " \"<class 'unstructured.documents.elements.Table'>\": 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This table provides an overview of the different sections and goals outlined in the document. It includes sections on problem statement, scope, vision statement, guiding principles, architecture, standards, governance, talent and culture, and goals related to making data visible, accessible, understandable, linked, trustworthy, interoperable, and secure. The document also mentions strengthened governance and focus areas.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Department of Defense (DoD) has released its Data Strategy, which aims to transform the department into a data-centric enterprise. The strategy focuses on areas such as joint operations, decision support, and business analytics. It also outlines eight guiding principles, including treating data as a strategic asset and ensuring data ethics. The strategy highlights four essential capabilities, including architecture and talent/culture. The DoD has set seven goals to become a data-centric organization, including making data visible, accessible, and understandable for consumers.',\n",
       " \"The DoD Data Strategy aims to improve data management in order to enhance the Department's ability to fight and win wars. The strategy includes making data linked, trustworthy, interoperable, and secure. To implement the strategy, measurable Data Strategy Implementation Plans will be developed and overseen by the DoD CDO and DoD Data Council. The data governance community and user communities will also collaborate to address challenges and share best practices. The adoption of new technologies and the secure use of data will enable the Department to gain a competitive advantage and improve military lethality.\",\n",
       " 'The table or text chunk is discussing the DoD Data Strategy, which aims to amass data resources, provide decision support, and transform efficiency in order to achieve data superiority and support modernization and digital capabilities. The strategy emphasizes treating data as a weapon system and highlights the importance of secure management and navigation of data for the U.S. military.',\n",
       " 'The DoD Data Strategy aims to enhance military effectiveness through access to accurate, timely, and secure data. It supports the National Defense Strategy and provides data-driven insights for DoD operations and management. The strategy emphasizes the need for seamless access to data across networks and platforms, open data standards, and coordination of information for superior situational awareness. The strategy includes a vision statement, guiding principles, essential capabilities, goals, and objectives.',\n",
       " 'The DoD Data Strategy aims to address the lack of enterprise data management within the Department of Defense. The strategy emphasizes the need for trusted, accessible, and secure data that can be used in real-time for decision-making. It also highlights the importance of data interoperability in software and hardware systems to enable faster and more efficient operations. The strategy calls for improving skills in data management and cultivating a culture of data awareness. The scope of the strategy applies to the entire Department of Defense and its data across all systems. The vision is for the DoD to become a data-centric organization that uses data for operational advantage and increased efficiency.',\n",
       " 'The Department of Defense (DoD) considers data to be a strategic asset and believes that managing data effectively can provide distinct advantages over competitors and adversaries. By leveraging data in a way that brings immediate and lasting military advantage, the DoD aims to make more rapid and better-informed decisions through the use of trustworthy and integrated data.',\n",
       " 'The DoD is implementing a data strategy that includes collective data stewardship, data ethics, data collection, and enterprise-wide data access and availability. Data stewards, data custodians, and functional data managers will be assigned to ensure accountability throughout the data lifecycle. Ethical principles regarding the responsible use of data will be prioritized, and data collection will be automated to minimize human error. The DoD aims to make data available for use by all authorized individuals and non-person entities, with data sharing being the default posture. Data for artificial intelligence training is also emphasized.',\n",
       " 'The Department of Defense (DoD) recognizes the importance of Artificial Intelligence (AI) and the need for high-quality training datasets (TQD) to build algorithmic models. These digital assets will be crucial in modernizing and integrating AI technologies into joint warfighting. The DoD Chief Data Officer (CDO) will work with DoD Components to create a governance framework for managing the lifecycle of algorithm models and associated data to protect and responsibly broker these assets. Data \"fit for purpose\" is emphasized, ensuring quality, discoverability, and ethical considerations. Compliance with data policies is essential for continued funding and authorizations to operate.',\n",
       " 'The table or text chunk discusses two essential capabilities needed to accomplish the DoD data goals: architecture and standards. The architecture capability emphasizes access to data and the ability to adjust requirements in stride with changes in technology and data sources. The standards capability involves employing a family of standards for the management and utilization of data assets, as well as representing and sharing data. These capabilities are necessary to enable the achievement of all goals, and their implementation will be addressed in future data strategy plans.',\n",
       " 'The DoD Data Strategy emphasizes the importance of data governance and the need for a cultural transformation within the Department to become a data-centric organization. Data governance involves managing data at all levels and resolving issues at the lowest level possible. It also includes localized system decisions and records management. The strategy aims to empower the DoD workforce to work with data, make data-informed decisions, and implement effective processes. The goals of the strategy are to make DoD data visible, accessible, understandable, linked, trustworthy, interoperable, and secure.',\n",
       " 'The goal of making data visible is to allow authorized users to easily discover and access data that is of interest or value. This involves identifying, registering, and exposing data in a way that makes it easily discoverable across the enterprise. The DoD will know it has made progress on this goal when data is advertised and available for authorized users, metadata standards are implemented, all data sources are catalogued, common services are implemented for publishing and discovering data, and governance bodies make decisions based on live visualizations of near real-time data.\\n\\nThe goal of making data accessible is to ensure that authorized users can obtain the data they need when they need it. This includes having data automatically pushed to interested and authorized users. Data accessibility must comply with relevant laws and regulations. The DoD will know it has made progress on this goal when data is accessible through documented standard APIs, common platforms and services are used to create, retrieve, share, utilize, and manage data, and data access and sharing is controlled through reusable APIs.',\n",
       " 'Objective 3: Data is linked across systems and platforms to enable seamless data sharing and integration.\\n\\nObjective 4: DoD establishes and maintains data catalogs that capture and document data relationships and dependencies.\\n\\nObjective 5: Data is tagged with metadata that describes its characteristics, enabling efficient searching and linking of related data.\\n\\nObjective 6: DoD leverages industry best-practices for open data standards to ensure interoperability and compatibility with external data sources.\\n\\nObjective 7: Data is integrated and connected to enable advanced analytics and insights.\\n\\nOverall, the goal of making data understandable and linked is to enable enhanced decision-making and improve the efficiency and effectiveness of the Department of Defense (DoD). This involves presenting data in a standardized manner, utilizing common data syntax and metadata, aligning data elements into a comprehensive dictionary, baselining and inventorying data, creating and managing business vocabularies, and implementing adaptive, intelligent systems. Additionally, making data linked involves implementing unique identifiers, common metadata standards, establishing data catalogs, tagging data with metadata, and leveraging industry best-practices for open data standards.',\n",
       " 'The Department of Defense (DoD) has set goals to make data trustworthy, interoperable, and secure. To achieve data trustworthiness, the DoD aims to integrate data-focused evidence into budget requests, protect data throughout its lifecycle, assess and enhance data quality, implement master data management, and properly tag and maintain data. For data interoperability, the DoD plans to develop data exchange specifications, include required metadata and standardized semantic meaning, make public data assets machine-readable, mediate differing data standards, and establish a data-tagging strategy. To ensure data security, the DoD aims to implement granular privilege management, regularly assess classification criteria, implement approved security standards, define and implement classification and control markings, prevent data loss, restrict access to authorized users, bind access and handling restriction metadata to data, and fully audit data access, use, and disposition.',\n",
       " \"The Department of Defense (DoD) is working on operationalizing its data strategy, with a focus on strengthening data governance. This includes increased oversight at multiple levels, with the Office of the DoD Chief Data Officer (CDO) governing data management efforts and the DoD Chief Information Officer (CIO) integrating data priorities into the DoD Digital Modernization program. The CDO Council, chaired by the DoD CDO, will collaborate with data officers from across the Department to identify and prioritize data challenges, develop solutions, and oversee policy and data standards. Data considerations will be integrated into all aspects of the Department's requirements, research, procurement, budgeting, and manpower decisions.\",\n",
       " \"The Department of Defense (DoD) recognizes that data policies and standards alone are not enough to improve data management and quality. Feedback from users, particularly from the operational community and warfighters, is crucial in informing and strengthening data practices. The DoD is focusing on several key areas, including Joint All-Domain Operations, business analytics, and senior leader decision support. In Joint All-Domain Operations, the DoD aims to coordinate military effects in an all-domain fight and ensure data standards and interoperability. The data governance community will work with mission area managers to address data gaps and determine necessary changes. Senior leader decision support involves developing clear metrics and tools for analysis and visualization to inform management decisions. The DoD Comptroller is leading an effort to analyze and display business data, which will inform data governance policies and potentially lead to a common data platform for business analytics across the Department. Overall, effective feedback and collaboration between the data governance and operational communities are essential for the DoD's transition to a data-centric organization.\",\n",
       " 'The DoD Data Strategy emphasizes the importance of developing measurable Data Strategy Implementation Plans to support the transformation of the Department into a data-driven organization. The CDO Council will provide support by establishing common metrics and measures of performance. This information will be used to track progress, identify milestones, and demonstrate data improvements to stakeholders. The strategy highlights the significance of data in digital modernization and emphasizes the need for strong data management and partnerships with users, particularly warfighters. It emphasizes the importance of treating data as a weapon system and providing personnel with the necessary skills and tools to maintain U.S. military advantage.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptTemplate = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"\"\"\n",
    "                    Decompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of context.\n",
    "                        1. Split compound sentence into simple sentences. Maintain the original phrasing from the input whenever possible.\n",
    "                        2. For any named entity that is accompanied by additional descriptive information, separate this information into its own distinct proposition.\n",
    "                        3. De-contextualize the proposition by adding necessary modifier to nouns or entire sentences and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the entities they refer to.\n",
    "                        4. Present the results as a list of strings, formatted in JSON.\n",
    "\n",
    "                    Example:\n",
    "\n",
    "                        Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content: The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in 1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in other parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were frequently seen in gardens in spring, and thus may have served as a convenient explanation for the\n",
    "                        origin of the colored eggs hidden there for children. Alternatively, there is a European tradition that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and both occur on grassland and are first seen in the spring. In the nineteenth century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe. German immigrants then exported the custom to Britain and America where it evolved into the Easter Bunny.\"\n",
    "                        Output: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in 1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of medicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until the 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about the possible explanation for the connection between hares and the tradition during Easter\", \"Hares\n",
    "                        were frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation for the origin of the colored eggs hidden in gardens for children.\", \n",
    "                        \"There is a European tradition that hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both hares and lapwing’s nests occur on grassland and are first seen in the spring.\", \n",
    "                        \"In the nineteenth century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to Britain and America.\", \n",
    "                        \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in Britain and America.\"]\n",
    "                    \"\"\",\n",
    "                ),\n",
    "                (\"user\", \"Decompose the following:\\n{input}\\n`\"),\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cvardema/miniforge3/envs/taitac/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key = os.getenv(\"OPENAI_API_KEY\", 'YouKey'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it in a runnable\n",
    "runnable = PromptTemplate | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic data class\n",
    "class Sentences(BaseModel):\n",
    "    sentences: List[str]\n",
    "    \n",
    "# Extraction\n",
    "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Roses are red because of the way their petals are structured and the pigments they contain. The red color of roses is due to the presence of a pigment called anthocyanin, which is a type of flavonoid. Anthocyanin is responsible for the red, purple, and blue colors found in many flowers, fruits, and vegetables.\n",
      "\n",
      "The structure of the rose petal itself also plays a role in its color. The outer layer of the petal, called the epidermis, contains cells that are filled with anthocyanin. These cells are arranged in layers, with the largest amount of pigment found in the outermost layer. This layer is exposed to the sun's light, which causes the anthocyanin to absorb certain wavelengths of light and reflect others, giving the rose its red color.\n",
      "\n",
      "The exact shade of red can vary depending on the type of rose and other factors such as the amount of sunlight it receives, the soil it's grown in, and the presence of other pigments. Some roses may have a more orange or yellow tint to their red, while others may be more intense and deep.\n",
      "\n",
      "It's worth noting that the red color of roses is not fixed and can change over time due to factors such as weather conditions, soil quality, and genetic mutations. For example, some roses may have a more yellow or orange tint in the spring and summer months, before the anthocyanin content increases as the plant matures.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why are roses red?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.embeddings(model='llama2', prompt='They sky is blue because of rayleigh scattering')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model = OllamaFunctions(model=\"dolphin-mixtral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success count: 25\n",
      "Exception count: 0\n",
      "Total time: 63.302969217300415\n",
      "Average time: 12.660593843460083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\"location\": \"Boston, MA\"}'}})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "success_count = 0\n",
    "exception_count = 0\n",
    "total_time = 0\n",
    "\n",
    "for _ in range(25):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = model.invoke(\"what is the weather in Boston in fahrenheit?\")\n",
    "        success_count += 1\n",
    "    except json.JSONDecodeError:\n",
    "        exception_count += 1\n",
    "    end_time = time.time()\n",
    "    total_time += end_time - start_time\n",
    "\n",
    "print(\"Success count:\", success_count)\n",
    "print(\"Exception count:\", exception_count)\n",
    "print(\"Total time:\", total_time)\n",
    "print(\"Average time:\", total_time / 5)\n",
    "\n",
    "model.invoke(\"what is the weather in Boston?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
